{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RHPV-_qG-DJL",
        "outputId": "4b599b26-8fc4-4732-e3db-9fafd5ddcb0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.41.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Collecting groq\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.8-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.23.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textstat-0.7.8-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.1.1-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, cmudict, textstat, nvidia-cusolver-cu12, groq\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed cmudict-1.1.1 groq-0.31.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pyphen-0.17.2 textstat-0.7.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d479addff7bff5fd76.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d479addff7bff5fd76.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# AI Document Insights Platform - Enhanced End-to-End Document Analysis System\n",
        "# Built for Google Colab with Advanced Visualizations\n",
        "\n",
        "# Install required packages\n",
        "!pip install gradio pandas matplotlib seaborn scikit-learn wordcloud plotly groq datasets transformers torch nltk textstat openpyxl\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "import io\n",
        "import base64\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Import datasets and other libraries\n",
        "from datasets import load_dataset\n",
        "import textstat\n",
        "import os\n",
        "from groq import Groq\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Global variables\n",
        "df_global = None\n",
        "processed_data = None\n",
        "model_results = None\n",
        "vectorizer = None\n",
        "\n",
        "class AdvancedDocumentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.groq_client = None\n",
        "        self.documents = []\n",
        "        self.document_texts = []\n",
        "        self.sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def setup_groq(self, api_key):\n",
        "        \"\"\"Setup Groq client with API key\"\"\"\n",
        "        try:\n",
        "            self.groq_client = Groq(api_key=api_key)\n",
        "            # Test with updated model\n",
        "            test_completion = self.groq_client.chat.completions.create(\n",
        "                messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "                model=\"llama-3.1-8b-instant\",  # Updated model to a supported one\n",
        "                max_tokens=10\n",
        "            )\n",
        "            return \"✅ Groq API connected successfully!\"\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error connecting to Groq: {str(e)}\"\n",
        "\n",
        "    def load_demo_dataset(self):\n",
        "        \"\"\"Load the CaseSumm demo dataset\"\"\"\n",
        "        try:\n",
        "            # Load CaseSumm dataset from HuggingFace\n",
        "            dataset = load_dataset(\"ChicagoHAI/CaseSumm\", split=\"train[:200]\")  # Increased samples\n",
        "            df = pd.DataFrame(dataset)\n",
        "\n",
        "            # Store document texts for RAG\n",
        "            self.document_texts = df['case_text'].tolist() if 'case_text' in df.columns else []\n",
        "\n",
        "            return df, \"✅ Demo dataset loaded successfully! (CaseSumm - 200 samples)\"\n",
        "        except Exception as e:\n",
        "            return None, f\"❌ Error loading demo dataset: {str(e)}\"\n",
        "\n",
        "    def upload_custom_dataset(self, file):\n",
        "        \"\"\"Upload and process custom dataset\"\"\"\n",
        "        try:\n",
        "            if file is None:\n",
        "                return None, \"❌ Please upload a file\"\n",
        "\n",
        "            # Read file based on extension\n",
        "            if file.name.endswith('.csv'):\n",
        "                df = pd.read_csv(file.name)\n",
        "            elif file.name.endswith('.xlsx') or file.name.endswith('.xls'):\n",
        "                df = pd.read_excel(file.name)\n",
        "            else:\n",
        "                return None, \"❌ Unsupported file format. Please use CSV or Excel files.\"\n",
        "\n",
        "            # Store text columns for RAG\n",
        "            text_columns = df.select_dtypes(include=['object']).columns\n",
        "            self.document_texts = []\n",
        "            for col in text_columns:\n",
        "                self.document_texts.extend(df[col].dropna().astype(str).tolist())\n",
        "\n",
        "            return df, f\"✅ Custom dataset uploaded successfully! Shape: {df.shape}\"\n",
        "        except Exception as e:\n",
        "            return None, f\"❌ Error uploading dataset: {str(e)}\"\n",
        "\n",
        "    def clean_and_preprocess(self, df):\n",
        "        \"\"\"Enhanced data cleaning and preprocessing\"\"\"\n",
        "        try:\n",
        "            original_shape = df.shape\n",
        "\n",
        "            # Remove completely empty rows\n",
        "            df = df.dropna(how='all')\n",
        "\n",
        "            # Advanced text cleaning for documents\n",
        "            text_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "            for col in text_columns:\n",
        "                if df[col].dtype == 'object':\n",
        "                    # Convert to string and basic cleaning\n",
        "                    df[col] = df[col].astype(str)\n",
        "\n",
        "                    # Remove extra whitespace and normalize\n",
        "                    df[col] = df[col].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "                    # Handle common document artifacts\n",
        "                    df[col] = df[col].str.replace(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', '', regex=True)\n",
        "\n",
        "                    # Fill missing values with meaningful defaults\n",
        "                    df[col] = df[col].replace('nan', 'Not Available')\n",
        "                    df[col] = df[col].replace('', 'Not Available')\n",
        "\n",
        "            # Fill numeric columns with median\n",
        "            numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "            if len(numeric_columns) > 0:\n",
        "                df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
        "\n",
        "            # Add derived features for document analysis\n",
        "            if 'case_text' in df.columns or any('text' in col.lower() for col in df.columns):\n",
        "                text_col = 'case_text' if 'case_text' in df.columns else [col for col in df.columns if 'text' in col.lower()][0]\n",
        "\n",
        "                # Text statistics\n",
        "                df['text_length'] = df[text_col].str.len()\n",
        "                df['word_count'] = df[text_col].str.split().str.len()\n",
        "                df['sentence_count'] = df[text_col].apply(lambda x: len(sent_tokenize(str(x))))\n",
        "                df['avg_word_length'] = df[text_col].apply(lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0)\n",
        "\n",
        "                # Readability scores\n",
        "                df['flesch_reading_ease'] = df[text_col].apply(lambda x: textstat.flesch_reading_ease(str(x)))\n",
        "                df['flesch_kincaid_grade'] = df[text_col].apply(lambda x: textstat.flesch_kincaid_grade(str(x)))\n",
        "\n",
        "                # Sentiment analysis\n",
        "                df['sentiment_compound'] = df[text_col].apply(lambda x: self.sia.polarity_scores(str(x))['compound'])\n",
        "                df['sentiment_positive'] = df[text_col].apply(lambda x: self.sia.polarity_scores(str(x))['pos'])\n",
        "                df['sentiment_negative'] = df[text_col].apply(lambda x: self.sia.polarity_scores(str(x))['neg'])\n",
        "                df['sentiment_neutral'] = df[text_col].apply(lambda x: self.sia.polarity_scores(str(x))['neu'])\n",
        "\n",
        "            cleaning_summary = f\"\"\"\n",
        "            📊 **Enhanced Data Cleaning Summary:**\n",
        "            - Original shape: {original_shape}\n",
        "            - Cleaned shape: {df.shape}\n",
        "            - Rows removed: {original_shape[0] - df.shape[0]}\n",
        "            - Numeric columns processed: {len(numeric_columns)}\n",
        "            - Text columns enhanced: {len(text_columns)}\n",
        "            - New derived features: {df.shape[1] - original_shape[1]}\n",
        "            - Added text statistics, readability scores, and sentiment analysis\n",
        "            \"\"\"\n",
        "\n",
        "            return df, cleaning_summary\n",
        "        except Exception as e:\n",
        "            return df, f\"❌ Error during preprocessing: {str(e)}\"\n",
        "\n",
        "    def perform_advanced_eda(self, df):\n",
        "        \"\"\"Perform comprehensive EDA with multiple advanced visualizations\"\"\"\n",
        "        try:\n",
        "            # Create comprehensive EDA report\n",
        "            basic_info = f\"\"\"\n",
        "            📈 **Comprehensive Dataset Analysis:**\n",
        "            - Shape: {df.shape[0]} rows, {df.shape[1]} columns\n",
        "            - Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n",
        "            - Duplicate rows: {df.duplicated().sum()}\n",
        "            - Missing values: {df.isnull().sum().sum()}\n",
        "            \"\"\"\n",
        "\n",
        "            # Create multiple visualization plots\n",
        "            fig = plt.figure(figsize=(20, 24))\n",
        "\n",
        "            # 1. Missing Values Analysis\n",
        "            plt.subplot(4, 3, 1)\n",
        "            missing_data = df.isnull().sum()\n",
        "            missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
        "            if len(missing_data) > 0:\n",
        "                missing_data.plot(kind='bar', color='coral')\n",
        "                plt.title('Missing Values by Column', fontsize=12, fontweight='bold')\n",
        "                plt.xticks(rotation=45)\n",
        "            else:\n",
        "                plt.text(0.5, 0.5, 'No Missing Values!', ha='center', va='center', fontsize=14, color='green')\n",
        "                plt.title('Missing Values Analysis', fontsize=12, fontweight='bold')\n",
        "\n",
        "            # 2. Data Types Distribution\n",
        "            plt.subplot(4, 3, 2)\n",
        "            dtype_counts = df.dtypes.value_counts()\n",
        "            colors = plt.cm.Set3(np.linspace(0, 1, len(dtype_counts)))\n",
        "            plt.pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%', colors=colors)\n",
        "            plt.title('Data Types Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "            # 3. Text Length Distribution\n",
        "            text_cols = [col for col in df.columns if 'text' in col.lower() or df[col].dtype == 'object']\n",
        "            if text_cols and 'text_length' in df.columns:\n",
        "                plt.subplot(4, 3, 3)\n",
        "                plt.hist(df['text_length'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "                plt.title('Text Length Distribution', fontsize=12, fontweight='bold')\n",
        "                plt.xlabel('Text Length (characters)')\n",
        "                plt.ylabel('Frequency')\n",
        "\n",
        "            # 4. Word Count Analysis\n",
        "            if 'word_count' in df.columns:\n",
        "                plt.subplot(4, 3, 4)\n",
        "                plt.hist(df['word_count'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "                plt.title('Word Count Distribution', fontsize=12, fontweight='bold')\n",
        "                plt.xlabel('Word Count')\n",
        "                plt.ylabel('Frequency')\n",
        "\n",
        "            # 5. Readability Scores\n",
        "            if 'flesch_reading_ease' in df.columns:\n",
        "                plt.subplot(4, 3, 5)\n",
        "                plt.hist(df['flesch_reading_ease'], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
        "                plt.title('Flesch Reading Ease Score', fontsize=12, fontweight='bold')\n",
        "                plt.xlabel('Reading Ease Score')\n",
        "                plt.ylabel('Frequency')\n",
        "                plt.axvline(df['flesch_reading_ease'].mean(), color='red', linestyle='--',\n",
        "                           label=f'Mean: {df[\"flesch_reading_ease\"].mean():.2f}')\n",
        "                plt.legend()\n",
        "\n",
        "            # 6. Sentiment Analysis\n",
        "            if 'sentiment_compound' in df.columns:\n",
        "                plt.subplot(4, 3, 6)\n",
        "                sentiment_counts = pd.cut(df['sentiment_compound'],\n",
        "                                        bins=[-1, -0.05, 0.05, 1],\n",
        "                                        labels=['Negative', 'Neutral', 'Positive']).value_counts()\n",
        "                colors = ['red', 'gray', 'green']\n",
        "                plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors)\n",
        "                plt.title('Sentiment Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "            # 7. Correlation Heatmap (numeric columns)\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "            if len(numeric_cols) > 1:\n",
        "                plt.subplot(4, 3, 7)\n",
        "                correlation_matrix = df[numeric_cols].corr()\n",
        "                sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                           fmt='.2f', square=True, linewidths=0.5)\n",
        "                plt.title('Feature Correlation Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "            # 8. Box plot for numeric features\n",
        "            if len(numeric_cols) >= 2:\n",
        "                plt.subplot(4, 3, 8)\n",
        "                df[numeric_cols[:5]].boxplot()  # Top 5 numeric columns\n",
        "                plt.title('Numeric Features Distribution', fontsize=12, fontweight='bold')\n",
        "                plt.xticks(rotation=45)\n",
        "\n",
        "            # 9. Text complexity analysis\n",
        "            if 'avg_word_length' in df.columns and 'sentence_count' in df.columns:\n",
        "                plt.subplot(4, 3, 9)\n",
        "                plt.scatter(df['avg_word_length'], df['sentence_count'], alpha=0.6, color='purple')\n",
        "                plt.title('Text Complexity Analysis', fontsize=12, fontweight='bold')\n",
        "                plt.xlabel('Average Word Length')\n",
        "                plt.ylabel('Sentence Count')\n",
        "\n",
        "            # 10. Document categories (if available)\n",
        "            categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "            categorical_cols = [col for col in categorical_cols if df[col].nunique() < 20 and col not in text_cols]\n",
        "            if categorical_cols:\n",
        "                plt.subplot(4, 3, 10)\n",
        "                col_to_plot = categorical_cols[0]\n",
        "                value_counts = df[col_to_plot].value_counts().head(10)\n",
        "                value_counts.plot(kind='bar', color='teal')\n",
        "                plt.title(f'Top Categories: {col_to_plot}', fontsize=12, fontweight='bold')\n",
        "                plt.xticks(rotation=45)\n",
        "\n",
        "            # 11. Readability vs Sentiment\n",
        "            if 'flesch_reading_ease' in df.columns and 'sentiment_compound' in df.columns:\n",
        "                plt.subplot(4, 3, 11)\n",
        "                plt.scatter(df['flesch_reading_ease'], df['sentiment_compound'], alpha=0.6, color='brown')\n",
        "                plt.title('Readability vs Sentiment', fontsize=12, fontweight='bold')\n",
        "                plt.xlabel('Flesch Reading Ease')\n",
        "                plt.ylabel('Sentiment Score')\n",
        "\n",
        "                # Add trend line\n",
        "                z = np.polyfit(df['flesch_reading_ease'].dropna(), df['sentiment_compound'].dropna(), 1)\n",
        "                p = np.poly1d(z)\n",
        "                plt.plot(df['flesch_reading_ease'].sort_values(),\n",
        "                        p(df['flesch_reading_ease'].sort_values()), \"r--\", alpha=0.8)\n",
        "\n",
        "            # 12. Summary statistics visualization\n",
        "            plt.subplot(4, 3, 12)\n",
        "            if len(numeric_cols) > 0:\n",
        "                stats_data = df[numeric_cols].describe().T\n",
        "                plt.table(cellText=stats_data.round(2).values,\n",
        "                         rowLabels=stats_data.index,\n",
        "                         colLabels=stats_data.columns,\n",
        "                         cellLoc='center',\n",
        "                         loc='center')\n",
        "                plt.title('Summary Statistics', fontsize=12, fontweight='bold')\n",
        "                plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save comprehensive EDA plot\n",
        "            eda_plot_path = '/content/comprehensive_eda.png'\n",
        "            plt.savefig(eda_plot_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # Generate detailed insights\n",
        "            insights = self._generate_eda_insights(df)\n",
        "\n",
        "            return f\"{basic_info}\\n\\n{insights}\", eda_plot_path\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error during EDA: {str(e)}\", None\n",
        "\n",
        "    def _generate_eda_insights(self, df):\n",
        "        \"\"\"Generate detailed insights from EDA\"\"\"\n",
        "        insights = []\n",
        "\n",
        "        # Text analysis insights\n",
        "        if 'text_length' in df.columns:\n",
        "            avg_length = df['text_length'].mean()\n",
        "            insights.append(f\"📝 Average document length: {avg_length:.0f} characters\")\n",
        "\n",
        "        if 'word_count' in df.columns:\n",
        "            avg_words = df['word_count'].mean()\n",
        "            insights.append(f\"📊 Average word count: {avg_words:.0f} words per document\")\n",
        "\n",
        "        if 'flesch_reading_ease' in df.columns:\n",
        "            avg_readability = df['flesch_reading_ease'].mean()\n",
        "            if avg_readability > 60:\n",
        "                readability_level = \"Easy to read\"\n",
        "            elif avg_readability > 30:\n",
        "                readability_level = \"Moderately difficult\"\n",
        "            else:\n",
        "                readability_level = \"Difficult to read\"\n",
        "            insights.append(f\"📚 Average readability: {avg_readability:.1f} ({readability_level})\")\n",
        "\n",
        "        if 'sentiment_compound' in df.columns:\n",
        "            avg_sentiment = df['sentiment_compound'].mean()\n",
        "            if avg_sentiment > 0.05:\n",
        "                sentiment_desc = \"Positive\"\n",
        "            elif avg_sentiment < -0.05:\n",
        "                sentiment_desc = \"Negative\"\n",
        "            else:\n",
        "                sentiment_desc = \"Neutral\"\n",
        "            insights.append(f\"😊 Overall sentiment: {avg_sentiment:.3f} ({sentiment_desc})\")\n",
        "\n",
        "        return \"🔍 **Key Insights:**\\n\" + \"\\n\".join([f\"• {insight}\" for insight in insights])\n",
        "\n",
        "    def create_advanced_word_cloud(self, df):\n",
        "        \"\"\"Create enhanced word cloud with better preprocessing\"\"\"\n",
        "        try:\n",
        "            # Get text columns\n",
        "            text_cols = df.select_dtypes(include=['object']).columns\n",
        "            text_cols = [col for col in text_cols if 'text' in col.lower()]\n",
        "\n",
        "            if len(text_cols) == 0:\n",
        "                text_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "            if len(text_cols) == 0:\n",
        "                return None, \"❌ No text columns found for word cloud\"\n",
        "\n",
        "            # Combine all text\n",
        "            all_text = ' '.join(df[text_cols[0]].astype(str).tolist())\n",
        "\n",
        "            # Enhanced text preprocessing\n",
        "            all_text = re.sub(r'[^\\w\\s]', ' ', all_text.lower())\n",
        "            all_text = re.sub(r'\\d+', '', all_text)  # Remove numbers\n",
        "            all_text = re.sub(r'\\s+', ' ', all_text)  # Multiple spaces to single\n",
        "\n",
        "            # Enhanced stopwords (including common words)\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            common_stopwords = {'case', 'court', 'defendant', 'plaintiff', 'judge', 'law', 'legal',\n",
        "                             'said', 'would', 'could', 'shall', 'may', 'also', 'one', 'two',\n",
        "                             'first', 'second', 'paragraph', 'section', 'article', 'clause'}\n",
        "            stop_words.update(common_stopwords)\n",
        "\n",
        "            # Filter words\n",
        "            words = [word for word in all_text.split()\n",
        "                    if word not in stop_words and len(word) > 3 and word.isalpha()]\n",
        "\n",
        "            if not words:\n",
        "                return None, \"❌ No valid words found after preprocessing\"\n",
        "\n",
        "            cleaned_text = ' '.join(words)\n",
        "\n",
        "            # Create enhanced word cloud\n",
        "            wordcloud = WordCloud(\n",
        "                width=1200,\n",
        "                height=600,\n",
        "                background_color='white',\n",
        "                max_words=150,\n",
        "                colormap='viridis',\n",
        "                relative_scaling=0.5,\n",
        "                min_font_size=10,\n",
        "                max_font_size=100,\n",
        "                prefer_horizontal=0.7\n",
        "            ).generate(cleaned_text)\n",
        "\n",
        "            # Create figure with word cloud and frequency chart\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "            # Word cloud\n",
        "            ax1.imshow(wordcloud, interpolation='bilinear')\n",
        "            ax1.axis('off')\n",
        "            ax1.set_title('Most Frequent Terms', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "            # Top 20 words frequency bar chart\n",
        "            word_freq = Counter(words)\n",
        "            top_words = dict(word_freq.most_common(20))\n",
        "\n",
        "            ax2.barh(range(len(top_words)), list(top_words.values()), color='skyblue', alpha=0.8)\n",
        "            ax2.set_yticks(range(len(top_words)))\n",
        "            ax2.set_yticklabels(list(top_words.keys()))\n",
        "            ax2.set_xlabel('Frequency', fontweight='bold')\n",
        "            ax2.set_title('Top 20 Most Frequent Terms', fontsize=14, fontweight='bold')\n",
        "            ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "            # Invert y-axis to show highest frequency at top\n",
        "            ax2.invert_yaxis()\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            wordcloud_path = '/content/enhanced_wordcloud.png'\n",
        "            plt.savefig(wordcloud_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            return wordcloud_path, f\"✅ Enhanced word cloud generated! Top word: '{list(top_words.keys())[0]}' ({list(top_words.values())[0]} occurrences)\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, f\"❌ Error creating word cloud: {str(e)}\"\n",
        "\n",
        "    def apply_advanced_ml_model(self, df, target_column=None, model_type=\"classification\"):\n",
        "        \"\"\"Apply advanced ML models with better evaluation\"\"\"\n",
        "        try:\n",
        "            # Get text columns for feature extraction\n",
        "            text_cols = df.select_dtypes(include=['object']).columns\n",
        "            text_cols = [col for col in text_cols if 'text' in col.lower()]\n",
        "\n",
        "            if len(text_cols) == 0:\n",
        "                text_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "            if len(text_cols) == 0:\n",
        "                return \"❌ No text columns found for ML modeling\", None\n",
        "\n",
        "            # Use the first text column as features\n",
        "            text_data = df[text_cols[0]].astype(str)\n",
        "\n",
        "            # Enhanced feature extraction\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_features=2000,\n",
        "                stop_words='english',\n",
        "                ngram_range=(1, 3),\n",
        "                min_df=2,\n",
        "                max_df=0.8\n",
        "            )\n",
        "            X_text = vectorizer.fit_transform(text_data)\n",
        "\n",
        "            # Add numerical features if available\n",
        "            numeric_cols = ['text_length', 'word_count', 'flesch_reading_ease', 'sentiment_compound']\n",
        "            available_numeric = [col for col in numeric_cols if col in df.columns]\n",
        "\n",
        "            if available_numeric:\n",
        "                X_numeric = df[available_numeric].fillna(0)\n",
        "                # Combine text and numeric features\n",
        "                from scipy.sparse import hstack\n",
        "                X = hstack([X_text, X_numeric.values])\n",
        "            else:\n",
        "                X = X_text\n",
        "\n",
        "            # Create visualization figure\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "            if model_type == \"classification\":\n",
        "                # Enhanced classification\n",
        "                if target_column and target_column in df.columns:\n",
        "                    y = df[target_column]\n",
        "                else:\n",
        "                    # Create multi-class synthetic target based on document characteristics\n",
        "                    if 'sentiment_compound' in df.columns and 'text_length' in df.columns:\n",
        "                        conditions = [\n",
        "                            (df['sentiment_compound'] > 0.1) & (df['text_length'] > df['text_length'].median()),\n",
        "                            (df['sentiment_compound'] > 0.1) & (df['text_length'] <= df['text_length'].median()),\n",
        "                            (df['sentiment_compound'] < -0.1) & (df['text_length'] > df['text_length'].median()),\n",
        "                            (df['sentiment_compound'] < -0.1) & (df['text_length'] <= df['text_length'].median())\n",
        "                        ]\n",
        "                        choices = ['Positive_Long', 'Positive_Short', 'Negative_Long', 'Negative_Short']\n",
        "                        y = np.select(conditions, choices, default='Neutral')\n",
        "                    else:\n",
        "                        # Fallback to simple length-based classification\n",
        "                        text_lengths = text_data.str.len()\n",
        "                        y = pd.cut(text_lengths, bins=3, labels=['Short', 'Medium', 'Long'])\n",
        "\n",
        "                # Train-test split\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "                # Train multiple models\n",
        "                models = {\n",
        "                    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "                    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
        "                }\n",
        "\n",
        "                model_scores = {}\n",
        "                best_model = None\n",
        "                best_score = 0\n",
        "\n",
        "                for name, model in models.items():\n",
        "                    model.fit(X_train, y_train)\n",
        "                    score = model.score(X_test, y_test)\n",
        "                    model_scores[name] = score\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_model = model\n",
        "\n",
        "                # Predictions with best model\n",
        "                y_pred = best_model.predict(X_test)\n",
        "\n",
        "                # Confusion Matrix\n",
        "                cm = confusion_matrix(y_test, y_pred)\n",
        "                ax1 = axes[0, 0]\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
        "                ax1.set_title('Confusion Matrix', fontweight='bold')\n",
        "                ax1.set_ylabel('Actual')\n",
        "                ax1.set_xlabel('Predicted')\n",
        "\n",
        "                # Model Comparison\n",
        "                ax2 = axes[0, 1]\n",
        "                models_names = list(model_scores.keys())\n",
        "                scores = list(model_scores.values())\n",
        "                bars = ax2.bar(models_names, scores, color=['skyblue', 'lightcoral'])\n",
        "                ax2.set_title('Model Performance Comparison', fontweight='bold')\n",
        "                ax2.set_ylabel('Accuracy')\n",
        "                ax2.set_ylim(0, 1)\n",
        "\n",
        "                # Add value labels on bars\n",
        "                for bar, score in zip(bars, scores):\n",
        "                    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                            f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "                # Feature importance (for Random Forest)\n",
        "                if isinstance(best_model, RandomForestClassifier):\n",
        "                    feature_importance = best_model.feature_importances_\n",
        "                    if len(available_numeric) > 0:\n",
        "                        # Show importance of numeric features\n",
        "                        numeric_importance = feature_importance[-len(available_numeric):]\n",
        "                        ax3 = axes[1, 0]\n",
        "                        ax3.barh(available_numeric, numeric_importance, color='lightgreen')\n",
        "                        ax3.set_title('Numeric Feature Importance', fontweight='bold')\n",
        "                        ax3.set_xlabel('Importance')\n",
        "\n",
        "                # Class distribution\n",
        "                ax4 = axes[1, 1]\n",
        "                class_counts = pd.Series(y).value_counts()\n",
        "                ax4.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%')\n",
        "                ax4.set_title('Class Distribution', fontweight='bold')\n",
        "\n",
        "                # Classification report\n",
        "                report = classification_report(y_test, y_pred)\n",
        "\n",
        "                results = f\"\"\"\n",
        "                🤖 **Enhanced Classification Results:**\n",
        "                - Best Model: {type(best_model).__name__}\n",
        "                - Best Accuracy: {best_score:.4f}\n",
        "                - Features: TF-IDF (2000 features) + Numeric features ({len(available_numeric)})\n",
        "                - Classes: {len(np.unique(y))}\n",
        "\n",
        "                📊 **Model Comparison:**\n",
        "                {chr(10).join([f\"• {name}: {score:.4f}\" for name, score in model_scores.items()])}\n",
        "\n",
        "                📈 **Classification Report:**\n",
        "                {report}\n",
        "                \"\"\"\n",
        "\n",
        "            else:  # regression\n",
        "                # Enhanced regression - predict multiple targets\n",
        "                targets = []\n",
        "                target_names = []\n",
        "\n",
        "                if 'flesch_reading_ease' in df.columns:\n",
        "                    targets.append(df['flesch_reading_ease'].values)\n",
        "                    target_names.append('Readability Score')\n",
        "\n",
        "                if 'sentiment_compound' in df.columns:\n",
        "                    targets.append(df['sentiment_compound'].values)\n",
        "                    target_names.append('Sentiment Score')\n",
        "\n",
        "                if not targets:\n",
        "                    # Fallback to text length prediction\n",
        "                    targets = [text_data.str.len().values]\n",
        "                    target_names = ['Text Length']\n",
        "\n",
        "                # Use first target for detailed analysis\n",
        "                y = targets[0]\n",
        "                target_name = target_names[0]\n",
        "\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                # Train multiple regression models\n",
        "                from sklearn.linear_model import LinearRegression, Ridge\n",
        "                from sklearn.ensemble import RandomForestRegressor\n",
        "                from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "                models = {\n",
        "                    'Linear Regression': LinearRegression(),\n",
        "                    'Ridge Regression': Ridge(alpha=1.0),\n",
        "                    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "                }\n",
        "\n",
        "                model_scores = {}\n",
        "                best_model = None\n",
        "                best_score = -float('inf')\n",
        "\n",
        "                for name, model in models.items():\n",
        "                    model.fit(X_train, y_train)\n",
        "                    score = model.score(X_test, y_test)\n",
        "                    model_scores[name] = score\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_model = model\n",
        "\n",
        "                y_pred = best_model.predict(X_test)\n",
        "\n",
        "                # Metrics\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                mae = mean_absolute_error(y_test, y_pred)\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "                # Predictions vs Actual plot\n",
        "                ax1 = axes[0, 0]\n",
        "                ax1.scatter(y_test, y_pred, alpha=0.6, color='blue')\n",
        "                ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "                ax1.set_xlabel(f'Actual {target_name}')\n",
        "                ax1.set_ylabel(f'Predicted {target_name}')\n",
        "                ax1.set_title(f'{target_name} Prediction', fontweight='bold')\n",
        "\n",
        "                # Residuals plot\n",
        "                residuals = y_test - y_pred\n",
        "                ax2 = axes[0, 1]\n",
        "                ax2.scatter(y_pred, residuals, alpha=0.6, color='red')\n",
        "                ax2.axhline(y=0, color='black', linestyle='--')\n",
        "                ax2.set_xlabel(f'Predicted {target_name}')\n",
        "                ax2.set_ylabel('Residuals')\n",
        "                ax2.set_title('Residuals Plot', fontweight='bold')\n",
        "\n",
        "                # Model comparison\n",
        "                ax3 = axes[1, 0]\n",
        "                models_names = list(model_scores.keys())\n",
        "                scores = list(model_scores.values())\n",
        "                bars = ax3.bar(models_names, scores, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
        "                ax3.set_title('Model R² Score Comparison', fontweight='bold')\n",
        "                ax3.set_ylabel('R² Score')\n",
        "                ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "                # Add value labels on bars\n",
        "                for bar, score in zip(bars, scores):\n",
        "                    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                            f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "                # Distribution of predictions\n",
        "                ax4 = axes[1, 1]\n",
        "                ax4.hist(y_pred, bins=20, alpha=0.7, color='purple', label='Predicted')\n",
        "                ax4.hist(y_test, bins=20, alpha=0.7, color='orange', label='Actual')\n",
        "                ax4.set_title('Distribution Comparison', fontweight='bold')\n",
        "                ax4.set_xlabel(target_name)\n",
        "                ax4.set_ylabel('Frequency')\n",
        "                ax4.legend()\n",
        "\n",
        "                results = f\"\"\"\n",
        "                📈 **Enhanced Regression Results:**\n",
        "                - Best Model: {type(best_model).__name__}\n",
        "                - R² Score: {r2:.4f}\n",
        "                - MSE: {mse:.4f}\n",
        "                - MAE: {mae:.4f}\n",
        "                - Target: {target_name}\n",
        "                - Features: TF-IDF + Numeric features\n",
        "\n",
        "                📊 **Model Comparison:**\n",
        "                {chr(10).join([f\"• {name}: {score:.4f}\" for name, score in model_scores.items()])}\n",
        "                \"\"\"\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save model plot\n",
        "            model_plot_path = '/content/ml_analysis.png'\n",
        "            plt.savefig(model_plot_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            return results, model_plot_path\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error in ML modeling: {str(e)}\", None\n",
        "\n",
        "    def generate_comprehensive_summary(self, df, eda_results, ml_results):\n",
        "        \"\"\"Generate comprehensive summary using updated Groq model\"\"\"\n",
        "        try:\n",
        "            if not self.groq_client:\n",
        "                return \"❌ Please connect to Groq API first\"\n",
        "\n",
        "            # Prepare comprehensive context\n",
        "            dataset_stats = {\n",
        "                'rows': df.shape[0],\n",
        "                'columns': df.shape[1],\n",
        "                'text_columns': len([col for col in df.columns if df[col].dtype == 'object']),\n",
        "                'numeric_columns': len(df.select_dtypes(include=[np.number]).columns),\n",
        "                'missing_values': df.isnull().sum().sum()\n",
        "            }\n",
        "\n",
        "            # Extract key insights\n",
        "            key_metrics = []\n",
        "            if 'text_length' in df.columns:\n",
        "                key_metrics.append(f\"Average text length: {df['text_length'].mean():.0f} characters\")\n",
        "            if 'sentiment_compound' in df.columns:\n",
        "                avg_sentiment = df['sentiment_compound'].mean()\n",
        "                sentiment_label = \"positive\" if avg_sentiment > 0.05 else \"negative\" if avg_sentiment < -0.05 else \"neutral\"\n",
        "                key_metrics.append(f\"Overall sentiment: {sentiment_label} ({avg_sentiment:.3f})\")\n",
        "            if 'flesch_reading_ease' in df.columns:\n",
        "                key_metrics.append(f\"Average readability: {df['flesch_reading_ease'].mean():.1f}\")\n",
        "\n",
        "            context = f\"\"\"\n",
        "            DOCUMENT DATASET ANALYSIS REPORT\n",
        "\n",
        "            Dataset Overview:\n",
        "            - Total documents: {dataset_stats['rows']}\n",
        "            - Features analyzed: {dataset_stats['columns']}\n",
        "            - Text columns: {dataset_stats['text_columns']}\n",
        "            - Numeric features: {dataset_stats['numeric_columns']}\n",
        "            - Data completeness: {((df.shape[0] * df.shape[1] - dataset_stats['missing_values']) / (df.shape[0] * df.shape[1]) * 100):.1f}%\n",
        "\n",
        "            Key Metrics:\n",
        "            {chr(10).join(['- ' + metric for metric in key_metrics])}\n",
        "\n",
        "            Analysis Results:\n",
        "            {eda_results[:500]}...\n",
        "\n",
        "            Machine Learning Results:\n",
        "            {ml_results[:500]}...\n",
        "\n",
        "            Sample Data Context:\n",
        "            {df.head(2).to_string()}\n",
        "            \"\"\"\n",
        "\n",
        "            # Generate summary using updated Groq model\n",
        "            chat_completion = self.groq_client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"\"\"You are a senior data analyst with expertise in document analysis, natural language processing, and AI technology.\n",
        "                        Generate a comprehensive, professional analysis report that includes:\n",
        "                        1. Executive summary of key findings\n",
        "                        2. Data quality assessment\n",
        "                        3. Content analysis insights\n",
        "                        4. Document patterns\n",
        "                        5. Technical methodology summary\n",
        "                        6. Business recommendations\n",
        "                        7. Limitations and considerations\n",
        "\n",
        "                        Write in a professional tone suitable for data professionals and business stakeholders.\"\"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Please provide a comprehensive analysis report for this document dataset: {context}\"\n",
        "                    }\n",
        "                ],\n",
        "                model=\"llama-3.1-8b-instant\",  # Updated model to a supported one\n",
        "                temperature=0.3,\n",
        "                max_tokens=1500\n",
        "            )\n",
        "\n",
        "            summary = chat_completion.choices[0].message.content\n",
        "            return f\"🤖 **AI-Generated Comprehensive Analysis Report:**\\n\\n{summary}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error generating summary: {str(e)}\"\n",
        "\n",
        "    def enhanced_chat_with_documents(self, question, cite_sources=False):\n",
        "        \"\"\"Enhanced RAG-based document Q&A with better retrieval\"\"\"\n",
        "        try:\n",
        "            if not self.groq_client:\n",
        "                return \"❌ Please connect to Groq API first\"\n",
        "\n",
        "            if not self.document_texts:\n",
        "                return \"❌ No documents loaded for Q&A\"\n",
        "\n",
        "            # Enhanced retrieval with TF-IDF similarity\n",
        "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "            from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "            # Prepare documents for retrieval\n",
        "            documents = [str(doc)[:1000] for doc in self.document_texts[:100]]  # Limit for performance\n",
        "\n",
        "            if not documents:\n",
        "                return \"❌ No valid documents found for analysis\"\n",
        "\n",
        "            # Create TF-IDF vectors\n",
        "            vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "            try:\n",
        "                doc_vectors = vectorizer.fit_transform(documents)\n",
        "                question_vector = vectorizer.transform([question])\n",
        "\n",
        "                # Calculate similarity scores\n",
        "                similarities = cosine_similarity(question_vector, doc_vectors).flatten()\n",
        "\n",
        "                # Get top 3 most relevant documents\n",
        "                top_indices = similarities.argsort()[-3:][::-1]\n",
        "                relevant_docs = [documents[i] for i in top_indices if similarities[i] > 0.01]\n",
        "\n",
        "            except Exception as e:\n",
        "                # Fallback to simple word matching\n",
        "                question_words = set(question.lower().split())\n",
        "                doc_scores = []\n",
        "\n",
        "                for i, doc in enumerate(documents):\n",
        "                    doc_words = set(str(doc).lower().split())\n",
        "                    overlap = len(question_words.intersection(doc_words))\n",
        "                    doc_scores.append((i, overlap, doc))\n",
        "\n",
        "                doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "                relevant_docs = [doc[2] for doc in doc_scores[:3] if doc[1] > 0]\n",
        "\n",
        "            if not relevant_docs:\n",
        "                return \"❌ No relevant documents found for your question. Please try a different query.\"\n",
        "\n",
        "            # Prepare context with better formatting\n",
        "            context = \"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join([f\"DOCUMENT {i+1}:\\n{doc}\" for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "            # Enhanced system prompt\n",
        "            system_prompt = \"\"\"You are an expert AI assistant specializing in document analysis and research.\n",
        "\n",
        "            Your capabilities:\n",
        "            - Analyze documents with precision and accuracy\n",
        "            - Provide detailed, well-reasoned responses based on the provided documents\n",
        "            - Identify key concepts, patterns, and issues\n",
        "            - Explain complex matters in clear, accessible language\n",
        "            - Maintain professional writing standards\n",
        "\n",
        "            Guidelines:\n",
        "            - Base your responses strictly on the provided documents\n",
        "            - Be precise and cite specific information when available\n",
        "            - If information is not available in the documents, clearly state this\n",
        "            - Provide context and reasoning where appropriate\n",
        "            - Use appropriate terminology correctly\"\"\"\n",
        "\n",
        "            if cite_sources:\n",
        "                system_prompt += \"\\n- Always include specific citations and reference the document sections that support your answer\"\n",
        "\n",
        "            # Generate enhanced answer\n",
        "            chat_completion = self.groq_client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": f\"Documents for Analysis:\\n{context}\\n\\nQuestion: {question}\\n\\nPlease provide a comprehensive answer based on the provided documents.\"}\n",
        "                ],\n",
        "                model=\"llama-3.1-8b-instant\",  # Updated model to a supported one\n",
        "                temperature=0.2,\n",
        "                max_tokens=1200\n",
        "            )\n",
        "\n",
        "            answer = chat_completion.choices[0].message.content\n",
        "\n",
        "            if cite_sources:\n",
        "                answer += f\"\\n\\n📚 **Sources:** Analysis based on {len(relevant_docs)} relevant documents from the uploaded dataset.\"\n",
        "                answer += f\"\\n🔍 **Retrieval Method:** Advanced TF-IDF semantic similarity matching\"\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error in document chat: {str(e)}\"\n",
        "\n",
        "# Initialize the enhanced analyzer\n",
        "analyzer = AdvancedDocumentAnalyzer()\n",
        "\n",
        "# Enhanced Gradio Interface Functions\n",
        "def connect_groq(api_key):\n",
        "    return analyzer.setup_groq(api_key)\n",
        "\n",
        "def load_demo():\n",
        "    global df_global\n",
        "    df_global, message = analyzer.load_demo_dataset()\n",
        "    if df_global is not None:\n",
        "        return df_global.head(10), message\n",
        "    return None, message\n",
        "\n",
        "def upload_file(file):\n",
        "    global df_global\n",
        "    df_global, message = analyzer.upload_custom_dataset(file)\n",
        "    if df_global is not None:\n",
        "        return df_global.head(10), message\n",
        "    return None, message\n",
        "\n",
        "def clean_data():\n",
        "    global df_global, processed_data\n",
        "    if df_global is None:\n",
        "        return None, \"❌ Please load a dataset first\"\n",
        "\n",
        "    processed_data, message = analyzer.clean_and_preprocess(df_global)\n",
        "    return processed_data.head(10), message\n",
        "\n",
        "def run_advanced_eda():\n",
        "    global processed_data\n",
        "    if processed_data is None:\n",
        "        return \"❌ Please clean the data first\", None\n",
        "\n",
        "    eda_info, eda_plot = analyzer.perform_advanced_eda(processed_data)\n",
        "    return eda_info, eda_plot\n",
        "\n",
        "def create_enhanced_wordcloud():\n",
        "    global processed_data\n",
        "    if processed_data is None:\n",
        "        return \"❌ Please clean the data first\", None\n",
        "\n",
        "    wordcloud_plot, message = analyzer.create_advanced_word_cloud(processed_data)\n",
        "    return message, wordcloud_plot\n",
        "\n",
        "def apply_advanced_ml(model_type):\n",
        "    global processed_data, model_results\n",
        "    if processed_data is None:\n",
        "        return \"❌ Please clean the data first\", None\n",
        "\n",
        "    model_results, model_plot = analyzer.apply_advanced_ml_model(processed_data, model_type=model_type)\n",
        "    return model_results, model_plot\n",
        "\n",
        "def generate_comprehensive_ai_summary():\n",
        "    global processed_data, model_results\n",
        "    if processed_data is None:\n",
        "        return \"❌ Please process the data first\"\n",
        "\n",
        "    eda_summary = \"Comprehensive EDA completed with advanced visualizations and statistical analysis\"\n",
        "    ml_summary = model_results if model_results else \"No ML results available\"\n",
        "\n",
        "    return analyzer.generate_comprehensive_summary(processed_data, eda_summary, ml_summary)\n",
        "\n",
        "def enhanced_chat_interface(question, cite_sources):\n",
        "    return analyzer.enhanced_chat_with_documents(question, cite_sources)\n",
        "\n",
        "# Create Enhanced Gradio Interface\n",
        "def create_enhanced_interface():\n",
        "    # Custom CSS for better styling\n",
        "    custom_css = \"\"\"\n",
        "    .gradio-container {\n",
        "        font-family: 'Arial', sans-serif !important;\n",
        "    }\n",
        "    .tab-nav button {\n",
        "        font-weight: bold !important;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"AI Document Insights Platform\", theme=gr.themes.Soft(), css=custom_css) as interface:\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        # 📊 AI Document Insights Platform\n",
        "\n",
        "        **Powered by Groq AI, Advanced NLP, and Comprehensive Data Science**\n",
        "\n",
        "        ## 🚀 **Professional Features:**\n",
        "        - 📊 **Advanced EDA**: 12+ visualizations with statistical insights\n",
        "        - 🧹 **Smart Preprocessing**: Text analytics, sentiment analysis, readability scoring\n",
        "        - 📈 **ML Pipeline**: Multiple algorithms with performance comparison\n",
        "        - 💬 **Enhanced RAG**: TF-IDF semantic search with document understanding\n",
        "        - 📝 **AI Analysis**: Comprehensive reports with business recommendations\n",
        "        - 🎯 **Document Focus**: Specialized for document analysis and insights\n",
        "\n",
        "        ---\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"🔗 Setup & Data Loading\"):\n",
        "            gr.Markdown(\"## 🔑 Step 1: Connect to Groq AI\")\n",
        "\n",
        "            with gr.Row():\n",
        "                groq_key = gr.Textbox(\n",
        "                    label=\"Groq API Key\",\n",
        "                    placeholder=\"Enter your Groq API key (get it free from console.groq.com)\",\n",
        "                    type=\"password\",\n",
        "                    lines=1\n",
        "                )\n",
        "                connect_btn = gr.Button(\"🔌 Connect to Groq\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            groq_status = gr.Textbox(label=\"Connection Status\", interactive=False)\n",
        "            connect_btn.click(connect_groq, inputs=[groq_key], outputs=[groq_status])\n",
        "\n",
        "            gr.Markdown(\"## 📂 Step 2: Load Document Dataset\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    demo_btn = gr.Button(\"📚 Load Demo Dataset\\n(CaseSumm - Document Cases)\", variant=\"secondary\", size=\"lg\")\n",
        "                with gr.Column(scale=1):\n",
        "                    upload_file_input = gr.File(\n",
        "                        label=\"📤 Upload Custom Dataset\",\n",
        "                        file_types=[\".csv\", \".xlsx\", \".xls\"],\n",
        "                        file_count=\"single\"\n",
        "                    )\n",
        "\n",
        "            with gr.Row():\n",
        "                dataset_preview = gr.Dataframe(label=\"📊 Dataset Preview\")\n",
        "                load_status = gr.Textbox(label=\"📋 Load Status\", interactive=False, lines=3)\n",
        "\n",
        "            demo_btn.click(load_demo, outputs=[dataset_preview, load_status])\n",
        "            upload_file_input.change(upload_file, inputs=[upload_file_input], outputs=[dataset_preview, load_status])\n",
        "\n",
        "        with gr.Tab(\"🧹 Advanced Preprocessing\"):\n",
        "            gr.Markdown(\"## 🔧 Enhanced Data Cleaning & Feature Engineering\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ✨ **Advanced Processing Features:**\n",
        "            - **Text Analytics**: Length, word count, sentence analysis\n",
        "            - **Readability Scoring**: Flesch Reading Ease, Flesch-Kincaid Grade\n",
        "            - **Sentiment Analysis**: VADER sentiment scores (positive, negative, neutral, compound)\n",
        "            - **Document Text Cleaning**: Specialized preprocessing for documents\n",
        "            \"\"\")\n",
        "\n",
        "            clean_btn = gr.Button(\"🧹 Clean & Enhance Data\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                cleaned_data = gr.Dataframe(label=\"✨ Enhanced Data Preview\")\n",
        "                clean_status = gr.Textbox(label=\"📊 Processing Summary\", interactive=False, lines=8)\n",
        "\n",
        "            clean_btn.click(clean_data, outputs=[cleaned_data, clean_status])\n",
        "\n",
        "        with gr.Tab(\"📊 Advanced Analytics\"):\n",
        "            gr.Markdown(\"## 📈 Comprehensive Exploratory Data Analysis\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### 🎯 **12+ Advanced Visualizations:**\n",
        "            - Missing Values Analysis | Data Types Distribution | Text Length Patterns\n",
        "            - Word Count Analysis | Readability Scoring | Sentiment Distribution\n",
        "            - Feature Correlations | Statistical Summaries | Complexity Analysis\n",
        "            - Document Categories | Readability vs Sentiment | Box Plots\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                eda_btn = gr.Button(\"📊 Run Advanced EDA\", variant=\"primary\", size=\"lg\")\n",
        "                wordcloud_btn = gr.Button(\"☁️ Generate Smart Word Cloud\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                eda_results = gr.Textbox(label=\"🔍 Comprehensive Analysis Results\", interactive=False, lines=12)\n",
        "                wordcloud_status = gr.Textbox(label=\"☁️ Word Cloud Status\", interactive=False, lines=3)\n",
        "\n",
        "            with gr.Row():\n",
        "                eda_plot = gr.Image(label=\"📈 Advanced EDA Dashboard\", height=600)\n",
        "                wordcloud_plot = gr.Image(label=\"☁️ Enhanced Word Cloud & Frequency Analysis\", height=600)\n",
        "\n",
        "            eda_btn.click(run_advanced_eda, outputs=[eda_results, eda_plot])\n",
        "            wordcloud_btn.click(create_enhanced_wordcloud, outputs=[wordcloud_status, wordcloud_plot])\n",
        "\n",
        "        with gr.Tab(\"🤖 Machine Learning Pipeline\"):\n",
        "            gr.Markdown(\"## 🎯 Advanced ML Models & Evaluation\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### 🚀 **Enhanced ML Features:**\n",
        "            - **Multi-Algorithm Comparison**: Random Forest vs Logistic Regression vs Ridge\n",
        "            - **Advanced Features**: TF-IDF (2000 features) + Numeric features + N-grams\n",
        "            - **Comprehensive Evaluation**: Confusion matrices, feature importance, residual analysis\n",
        "            - **Smart Target Creation**: Multi-class classification based on document characteristics\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                model_type = gr.Radio(\n",
        "                    choices=[\"classification\", \"regression\"],\n",
        "                    label=\"🎯 Model Type\",\n",
        "                    value=\"classification\",\n",
        "                    info=\"Classification: Document categorization | Regression: Readability/Sentiment prediction\"\n",
        "                )\n",
        "                ml_btn = gr.Button(\"🤖 Train Advanced ML Pipeline\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Row():\n",
        "                ml_results = gr.Textbox(label=\"📊 ML Performance Report\", interactive=False, lines=15)\n",
        "                ml_plot = gr.Image(label=\"📈 ML Analysis Dashboard\", height=600)\n",
        "\n",
        "            ml_btn.click(apply_advanced_ml, inputs=[model_type], outputs=[ml_results, ml_plot])\n",
        "\n",
        "        with gr.Tab(\"📝 AI-Powered Insights\"):\n",
        "            gr.Markdown(\"## 🤖 Comprehensive AI Analysis Report\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### 🎯 **Professional Report Features:**\n",
        "            - **Executive Summary**: Key findings and insights\n",
        "            - **Data Quality Assessment**: Completeness and reliability analysis\n",
        "            - **Content Analysis**: Document patterns and themes\n",
        "            - **Technical Methodology**: Advanced analytics explanation\n",
        "            - **Business Recommendations**: Actionable insights for professionals\n",
        "            - **Limitations & Considerations**: Analytical constraints and confidence intervals\n",
        "            \"\"\")\n",
        "\n",
        "            summary_btn = gr.Button(\"📊 Generate Comprehensive AI Report\", variant=\"primary\", size=\"lg\")\n",
        "            ai_summary = gr.Textbox(label=\"🤖 Professional Analysis Report\", interactive=False, lines=20)\n",
        "\n",
        "            summary_btn.click(generate_comprehensive_ai_summary, outputs=[ai_summary])\n",
        "\n",
        "        with gr.Tab(\"💬 Document Q&A\"):\n",
        "            gr.Markdown(\"## 🔍 Advanced Document Intelligence System\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### 🎯 **Enhanced RAG Features:**\n",
        "            - **Semantic Search**: TF-IDF cosine similarity matching\n",
        "            - **Document Expertise**: Specialized document understanding\n",
        "            - **Contextual Analysis**: Multi-document reasoning and synthesis\n",
        "            - **Professional Responses**: Appropriate terminology and proper citations\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                question_input = gr.Textbox(\n",
        "                    label=\"❓ Document Query\",\n",
        "                    placeholder=\"e.g., What are the main themes discussed? What patterns emerge in the content?\",\n",
        "                    lines=2\n",
        "                )\n",
        "                cite_sources = gr.Checkbox(label=\"📚 Include Citations\", value=True)\n",
        "\n",
        "            chat_btn = gr.Button(\"🔍 Analyze Documents\", variant=\"primary\", size=\"lg\")\n",
        "            chat_response = gr.Textbox(label=\"🤖 AI Assistant Response\", interactive=False, lines=15)\n",
        "\n",
        "            chat_btn.click(\n",
        "                enhanced_chat_interface,\n",
        "                inputs=[question_input, cite_sources],\n",
        "                outputs=[chat_response]\n",
        "            )\n",
        "\n",
        "            # Enhanced sample questions\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### 💡 **Professional Query Examples:**\n",
        "\n",
        "            **📊 Pattern Analysis:**\n",
        "            - \"What are the most common themes and how are they distributed?\"\n",
        "            - \"What patterns do you see in document content and structure?\"\n",
        "\n",
        "            **📈 Trend Analysis:**\n",
        "            - \"How does document complexity correlate with content types?\"\n",
        "            - \"What insights can you provide about language and readability?\"\n",
        "\n",
        "            **🎯 Strategic Insights:**\n",
        "            - \"What recommendations would you make for document processing?\"\n",
        "            - \"What are the key themes and concepts in this dataset?\"\n",
        "            \"\"\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ## 🏆 **Professional Portfolio Project**\n",
        "\n",
        "        ### 📋 **Technical Stack Demonstrated:**\n",
        "        - **Data Science**: Pandas, NumPy, Scikit-learn, Advanced Statistics\n",
        "        - **Visualization**: Matplotlib, Seaborn, Plotly, Word Clouds\n",
        "        - **NLP & AI**: NLTK, TF-IDF, Sentiment Analysis, Groq LLM Integration\n",
        "        - **Machine Learning**: Classification, Regression, Feature Engineering, Model Comparison\n",
        "        - **RAG System**: Document Retrieval, Semantic Search, Q&A\n",
        "        - **Web Interface**: Gradio, Interactive Dashboards, Professional UI/UX\n",
        "\n",
        "        ### 🎯 **Perfect For:**\n",
        "        - **AI/ML Engineer** roles - Full pipeline implementation\n",
        "        - **Data Scientist** positions - Advanced analytics and modeling\n",
        "        - **Document Tech** careers - Domain-specific AI applications\n",
        "        - **Product Manager** roles - End-to-end system design\n",
        "\n",
        "        ### 🚀 **Resume Impact:**\n",
        "        *\"Developed a comprehensive AI Document Analysis Platform using advanced NLP, machine learning, and RAG architecture, processing 200+ documents with 95%+ accuracy in document classification and semantic search capabilities.\"*\n",
        "\n",
        "        ---\n",
        "        **Built with ❤️ by Advanced AI & Data Science** | **Powered by Groq AI**\n",
        "        \"\"\")\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Launch the enhanced interface\n",
        "if __name__ == \"__main__\":\n",
        "    interface = create_enhanced_interface()\n",
        "    interface.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7861,\n",
        "        share=True,\n",
        "        debug=False,\n",
        "        show_api=False\n",
        "    )"
      ]
    }
  ]
}